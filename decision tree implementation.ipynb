{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3792b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80264d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183797d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "#Function to convert a continuous data into labelled data\n",
    "#There are 4 lables  - a, b, c, d\n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9759bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sl_labeled'] = toLabel(df, 'sl')\n",
    "df['sw_labeled'] = toLabel(df, 'sw')\n",
    "df['pl_labeled'] = toLabel(df, 'pl')\n",
    "df['pw_labeled'] = toLabel(df, 'pw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5788b7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)\n",
    "set(df['sl_labeled'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d438063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, data,output):\n",
    "        # data is the feature upon which the node is going to split when fitting the training data and it is none for leaf node.\n",
    "        self.data = data\n",
    "        self.children = {} #dictionary as mentioned above\n",
    "        # output represents the class with current majority at this instance of the decision tree\n",
    "        self.output = output\n",
    "        # \"index\" variable assigns a perticular index to each node\n",
    "        self.index = -1\n",
    "        \n",
    "    def add_child(self,feature_value,object):\n",
    "        self.children[feature_value] = object\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63acc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_tree:\n",
    "    def __init__(self):\n",
    "        # root node of our decision tree\n",
    "        self.__root = None\n",
    "\n",
    "    def __frequency_counter(self,Y):\n",
    "        # returns a dictionary with keys as unique values of Y(i.e no of classes) and the corresponding value as its frequency\n",
    "        d = {}\n",
    "        for i in Y:\n",
    "            if i in d:\n",
    "                d[i]+=1\n",
    "            else:\n",
    "                d[i]=1\n",
    "        return d\n",
    "\n",
    "\n",
    "    def __entropy(self,Y):\n",
    "        # returns the entropy of that perticular node.\n",
    "        freq_map = self.__frequency_counter(Y)\n",
    "        entropy_ = 0\n",
    "        total = len(Y)\n",
    "        for i in freq_map:\n",
    "            p = freq_map[i]/total\n",
    "            entropy_ += (-p)*math.log2(p)\n",
    "            #note that i have used log2 and not log10.\n",
    "        return entropy_\n",
    "\n",
    "    def __gain_ratio(self,X,Y,selected_feature):\n",
    "        # returns the gain ratio\n",
    "        original = self.__entropy(Y) # \"original\" represents entropy before splitting\n",
    "        entropy_after_splitting = 0 \n",
    "        split_info = 0\n",
    "        values = set(X[:,selected_feature])\n",
    "        df = pd.DataFrame(X)\n",
    "        # Appending Y values as the last column in the newely created dataframe \n",
    "        df[df.shape[1]] = Y\n",
    "        starting_size = df.shape[0] \n",
    "        for i in values:\n",
    "            df1 = df[df[selected_feature] == i]\n",
    "            current_size = df1.shape[0]\n",
    "            entropy_after_splitting += (current_size/starting_size)*self.__entropy(df1[df1.shape[1]-1])\n",
    "            split_info += (-current_size/starting_size)*math.log2(current_size/starting_size)\n",
    "\n",
    "        # when split info is zero, it will give an error. to handle that we have following piece of code\n",
    "        if split_info == 0 :\n",
    "            return math.inf \n",
    "        #returning negative infinity when split info is 0\n",
    "\n",
    "        info_gain = original - entropy_after_splitting\n",
    "        gain_ratio = info_gain / split_info\n",
    "        return gain_ratio #returned the gain_ratio finally\n",
    "\n",
    "\n",
    "\n",
    "    def __decision_tree(self,X,Y,features,level,classes, all_features=np.array([i for i in df.columns])):\n",
    "        # returns the root of the Decision Tree built after fitting the training data\n",
    "        # classes = all the different classes available to us\n",
    "        # level = depth of the tree\n",
    "        # traversal will be in preorder fashion\n",
    "        \n",
    "#                 If we run out of features to split upon, then the answer will be the class whose count will be the maximum for the given split\n",
    "#         If the length of unused features is zero, you will have to:\n",
    "#         1. print the level\n",
    "#         2. count the frequency of each class in Y, and store it in a dictionary, lets name it freqs\n",
    "#         3. now iterate in each of these unique classes and if you don't find that class in freqs then print \"count of (class_name) =0\" else if you find that particular class in the freqs dictionary, for this case just initialize two variables outside the for loop claaed output and maxcount. maxcount will store the count of the frequency of the class that has the maximum frequency in the freqs dict.\n",
    "\n",
    "        #BASE CASES\n",
    "        # first stopping criteria: If we have run out of features to split upon\n",
    "        # In this case the answer will be the class whose count is maximum for the given split\n",
    "        if len(features) == 0:\n",
    "            print(\"Level\",level)\n",
    "            freqs = self.__frequency_counter(Y)\n",
    "            output = None\n",
    "            max_count = -99999999999999\n",
    "            for i in classes:\n",
    "                if i not in freqs:\n",
    "                    print(\"Count of\",i,\"=\",0)\n",
    "                else :\n",
    "                    if freqs[i] > max_count :\n",
    "                        output = i\n",
    "                        max_count = freqs[i]\n",
    "                    print(\"Count of\",i,\"=\",freqs[i])\n",
    "\n",
    "            print(\"Current Entropy  is =\",self.__entropy(Y))          \n",
    "\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "        \n",
    "        \n",
    "        # second stopping criteria: If the node consists of a single pure class\n",
    "        if len(set(Y)) == 1:\n",
    "            print(\"Level\",level)\n",
    "            output = None\n",
    "            for class_ in classes:\n",
    "                if class_ in Y:\n",
    "                    output = class_\n",
    "                    print(\"Count of\",class_,\"=\",len(Y))\n",
    "                else :\n",
    "                    print(\"Count of\",class_,\"=\",0)\n",
    "            print(\"Current Entropy is =  0.0\")\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "\n",
    "        #now starts the backbone of our decision tree classifier.\n",
    "        # after the base cases we have to Find the best feature to split upon\n",
    "        max_gain = -math.inf\n",
    "        final_feature = None\n",
    "        for f in features :\n",
    "            current_gain = self.__gain_ratio(X,Y,f)\n",
    "\n",
    "            if current_gain > max_gain:\n",
    "                max_gain = current_gain\n",
    "                final_feature = f\n",
    "\n",
    "        print(\"Level\",level)\n",
    "        freqs = self.__frequency_counter(Y)\n",
    "        output = None\n",
    "        max_count = -math.inf\n",
    "\n",
    "        for class_ in classes:\n",
    "            if class_ not in freqs:\n",
    "                print(\"Count of\",class_,\"=\",0)\n",
    "            else :\n",
    "                if freqs[class_] > max_count :\n",
    "                    output = class_\n",
    "                    max_count = freqs[class_]\n",
    "                print(\"Count of\",class_,\"=\",freqs[class_])\n",
    "     \n",
    "        print(\"Current Entropy is =\",self.__entropy(Y))\n",
    "        print(\"Splitting on feature\" , all_features[final_feature] , \"with gain ratio\" , max_gain)\n",
    "        print()\n",
    "        \n",
    "\n",
    "            \n",
    "        unique_values = set(X[:,final_feature]) # unique_values represents the unique values of the feature selected\n",
    "        df = pd.DataFrame(X)\n",
    "        # appending y values at the end of the data frame.\n",
    "        df[df.shape[1]] = Y\n",
    "\n",
    "        current_node = TreeNode(final_feature,output)\n",
    "\n",
    "        # removing the selected feature from the list.\n",
    "        index  = features.index(final_feature)\n",
    "        features.remove(final_feature)\n",
    "        for i in unique_values:\n",
    "            # Creating a new dataframe with value of selected feature = i\n",
    "            df_new = df[df[final_feature] == i]\n",
    "            # recursively calling on the splits\n",
    "            node = self.__decision_tree(df_new.iloc[:,0:df_new.shape[1]-1].values,df_new.iloc[:,df_new.shape[1]-1].values,features,level+1,classes)\n",
    "            current_node.add_child(i,node)\n",
    "\n",
    "        # Add the removed feature     \n",
    "        features.insert(index,final_feature)\n",
    "\n",
    "        return current_node\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        # Fits according to the given training data\n",
    "        features = [i for i in range(len(X[0]))]\n",
    "        classes = set(Y)\n",
    "        level = 0\n",
    "        self.__root = self.__decision_tree(X,Y,features,level,classes)\n",
    "        \n",
    "    def __predict_for_single_point(self,data,node):\n",
    "        # predicts the class for a given testing point\n",
    "        # We have reached a leaf node\n",
    "        if len(node.children) == 0 :\n",
    "            return node.output\n",
    "\n",
    "        val = data[node.data] # represents the value of feature on which the split is made       \n",
    "        if val not in node.children :\n",
    "            return node.output\n",
    "        \n",
    "        # Recursively call on the splits\n",
    "        return self.__predict_for_single_point(data,node.children[val])\n",
    "\n",
    "    def predict(self,X):\n",
    "        # This function returns Y-predicted\n",
    "        # X is a 2D numpy array\n",
    "        Y = np.array([0 for i in range(len(X))])\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = self.__predict_for_single_point(X[i],self.__root)\n",
    "        return Y\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        # this function returns the mean accuracy.\n",
    "        Y_predicted = self.predict(X)\n",
    "        counter = 0\n",
    "        for i in range(len(Y_predicted)):\n",
    "            if Y_predicted[i] == Y[i]:\n",
    "                counter+=1\n",
    "        return counter/len(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beba4489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 37\n",
      "Count of 1 = 37\n",
      "Count of 2 = 38\n",
      "Current Entropy is = 1.5848478277058315\n",
      "Splitting on feature pw_labeled with gain ratio 0.6953554286060378\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 7\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 37\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 30\n",
      "Count of 2 = 13\n",
      "Current Entropy is = 0.8841151220488479\n",
      "Splitting on feature pl_labeled with gain ratio 0.43578224332795995\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 29\n",
      "Count of 2 = 6\n",
      "Current Entropy is = 0.6609623351442084\n",
      "Splitting on feature sl_labeled with gain ratio 0.14371916058923703\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 12\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 15\n",
      "Count of 2 = 5\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature sw_labeled with gain ratio 0.07405922160560252\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 9\n",
      "Count of 2 = 4\n",
      "Current Entropy  is = 0.8904916402194913\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 1\n",
      "Current Entropy  is = 0.9182958340544896\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 0\n",
      "Count of 1 = 4\n",
      "Count of 2 = 0\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 7\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 25\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "\n",
      "Score= 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=df.values\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y)\n",
    "unused_features = set(df.columns)\n",
    "clf=build_tree()\n",
    "clf.fit(x_train, y_train)\n",
    "print()\n",
    "print(\"Score=\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy-> 0.947"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
